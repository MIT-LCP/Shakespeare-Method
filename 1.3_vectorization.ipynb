{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Cleaning, collocation, count vectorization\n",
    "+ run on AWS instance or something with a large amt of ram (approx 109Gb)\n",
    "+ instance details (r4.8xlarge, us-east-1a, AMD64, 244GB memory, Windows 10)\n",
    "\n",
    "1 remove PII strings\n",
    "\n",
    "2 custom tokenizer to create vocabulary using gensim's phraser fxn with the following settings\n",
    "\n",
    "            ngrams 1-5\n",
    "            threshold = 2 min score for an n-gram to be taken into account  (depends on scorer), \n",
    "                higher number = less phrases\n",
    "            min_count = 2 ignore all words/phrases with a count lower than this\n",
    "            add additional vocab terms from the term collapsing in Naive bayes processing \n",
    "3 vectorize the data using count vectorizer  to create our document-term matrix\n",
    "        most parameters for count vect are ignored b/c we pass a vocabulary that is created in the `TextFeatures` class below. \n",
    "4 save as sparse pickle files that you can then import to a local machine for analysis\n",
    "\n",
    "\n",
    "+ postgres >= 9.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('last ran: ', datetime.datetime(2019, 8, 26, 1, 14, 37, 433000))\n",
      "('Python Version:', '2.7.14 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:34:40) [MSC v.1500 64 bit (AMD64)]')\n",
      "('operating system:', 'win32')\n",
      "Numpy Version: 1.14.2\n",
      "Pandas Version: 0.22.0\n",
      "Sklearn Version: 0.19.1\n",
      "Gensim Version: 3.1.0\n",
      "sqlalchemy Version: 1.1.13\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://postgres:postgrespassword@localhost/mimic')\n",
    "\n",
    "# print the versions of modules used here \n",
    "\n",
    "libraries = (('Numpy', np), ('Pandas', pd),('Sklearn', sklearn),('Gensim',gensim),('sqlalchemy',sqlalchemy))\n",
    "print('last ran: ',datetime.now() )\n",
    "print(\"Python Version:\", sys.version)\n",
    "print( \"operating system:\", sys.platform)\n",
    "for lib in libraries:\n",
    "    print('{0} Version: {1}'.format(lib[0], lib[1].__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions  \n",
    "+ **remove_masked_pii_string**: \n",
    "        the masked strings that identify individual patients or caregivers\n",
    "+ **tokenize_and_phrase**:\n",
    "        1. lowercase characters\n",
    "        2. remove newline characters (\\n)\n",
    "        3. tokenize: split into words on whitespace\n",
    "        4. remove empty tokens\n",
    "        5. collocation to find multi-word tokens (create n-grams (1-5))\n",
    "    \n",
    "+ **get_ngram_counts**:\n",
    "        vectorize (using count vectorizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextFeatures(object):\n",
    "    def __init__(self, corpus, pg_conn=None):\n",
    "\n",
    "        self.count_matrix = None\n",
    "        self.vocab = None\n",
    "        self.corpus = corpus\n",
    "    \n",
    "    def remove_masked_pii_string(self):\n",
    "        ''' using regular expression, remove the masked strings that identify individual patients or caregivers.'''\n",
    "        \n",
    "        num_reports = len(self.corpus)\n",
    "        for i in range(num_reports):\n",
    "            self.corpus[i] = re.sub(re.compile(\"\\[\\*\\*(.*?)\\*\\*\\]\"), \"\", self.corpus[i])\n",
    "    \n",
    "    def tokenize_and_phrase(self):        \n",
    "        print 'Creating tokens'\n",
    "        # tokenize: remove newline chars, lowercase, split into words on whitespace\n",
    "        sentence_stream = [doc.replace('\\n', ' ').lower().split(\" \") for doc in self.corpus]\n",
    "        \n",
    "        #remove empty tokens\n",
    "        for i in range(0,len(sentence_stream)):\n",
    "            sentence_stream[i] =[w for w in sentence_stream[i] if w !=u'']\n",
    "        print \"Done with tokens\"\n",
    "\n",
    "        # collocation detection\n",
    "        # This doesn't work as expected from the docs -- trigram creation actually creates up to 4 or 5 grams\n",
    "        bigram = Phraser(Phrases(sentence_stream, min_count=2,threshold=2, delimiter=' '))\n",
    "        print datetime.now()\n",
    "\n",
    "        trigram = Phraser(Phrases(bigram[sentence_stream], min_count=2,threshold=2, delimiter=' '))\n",
    "        print('trigram done')\n",
    "        print datetime.now()\n",
    "\n",
    "        tetragram = Phraser(Phrases(trigram[bigram[sentence_stream]], min_count=2,threshold=2, delimiter=' '))\n",
    "        print 'tetragram done'\n",
    "        print datetime.now()\n",
    "\n",
    "        pentagram = Phraser(Phrases(tetragram[trigram[bigram[sentence_stream]]], min_count=2,threshold=2, delimiter=' '))\n",
    "        print 'pentagram done'\n",
    "        print datetime.now()\n",
    "        \n",
    "        #=====================================================================================================\n",
    "        # Work around for unexpected behavior to reduce the gram count < 5\n",
    "        bigrams=[gram for sent in list(bigram[sentence_stream]) for gram in sent if len(gram.split(\" \")) == 2]\n",
    "        \n",
    "        print 'bigrams done' \n",
    "        print datetime.now()\n",
    "        \n",
    "        trigrams=[gram for sent in list(trigram[bigram[sentence_stream]]) for gram in sent if len(gram.split(\" \")) == 3]\n",
    "        \n",
    "        print 'trigrams done'\n",
    "        print datetime.now()\n",
    "        \n",
    "        tetragrams=[gram for sent in list(tetragram[trigram[bigram[sentence_stream]]]) for gram in sent if len(gram.split(\" \")) == 4]\n",
    "        \n",
    "        print 'tetragrams done'\n",
    "        print datetime.now()\n",
    "        \n",
    "        pentagrams=[gram for sent in list(pentagram[tetragram[trigram[bigram[sentence_stream]]]]) for gram in sent if len(gram.split(\" \")) == 5]\n",
    "        \n",
    "        print datetime.now()\n",
    "        \n",
    "        unigrams=[w for s in sentence_stream for w in s]\n",
    "        \n",
    "        print len(set(unigrams))\n",
    "        print datetime.now()\n",
    "        \n",
    "        vocab = list(set(unigrams+bigrams+trigrams+tetragrams+pentagrams))\n",
    "       \n",
    "        print(len(set(vocab)))\n",
    "        print \"Done with collocation detection\"\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    def get_ngram_counts(self, token_pattern, vocabulary):\n",
    "        '''Vectorizer (count) OUTPUT: vectorized data, vocabulary(feature names)'''\n",
    "        \n",
    "        print 'Starting feature extraction'\n",
    "        \n",
    "        count_vectorizer = CountVectorizer(token_pattern=token_pattern, vocabulary=vocabulary)\n",
    "        ngram_matrix = count_vectorizer.fit_transform(self.corpus)\n",
    "        ngram_vocab = count_vectorizer.get_feature_names()\n",
    "        \n",
    "        print(count_vectorizer)\n",
    "        return ngram_matrix, ngram_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save \n",
    "+ the large matricies from the analysis as compressed sparse row matricies\n",
    "+ chunk the vectorized data in pieces as pickle files so we can move them/store easily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_pickle(r, path):\n",
    "    t=time.strftime(\"%Y%m%d%H%m\",time.localtime())\n",
    "    if not os.path.exists(path+'\\\\'+t):\n",
    "        os.mkdir(path+'\\\\'+t)\n",
    "    path=path+'\\\\'+t+'\\\\'   \n",
    "    \n",
    "    print( r[0].shape)\n",
    "    chunk = (r[0].shape[0])/10\n",
    "    for i in range(0,9):\n",
    "        with open(path+'textfeatures_mat'+str(i+1)+'.pickle', 'wb') as f:\n",
    "            pickle.dump(r[0][chunk*i:chunk*(i+1)], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(path+'textfeatures_mat10.pickle', 'wb') as f:\n",
    "        pickle.dump(r[0][chunk*9:], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(path+'textfeatures_vocab.pickle', 'wb') as f:\n",
    "        pickle.dump(r[1], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(path+'textfeatures_id.pickle', 'wb') as f:\n",
    "        pickle.dump(r[2], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(path+'textfeatures_source.pickle', 'wb') as f:\n",
    "        pickle.dump(r[3], f, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data from Postgres\n",
    "+ pull notes and hadm_id and label as transfused or non-transfused (control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_adm_notes():\n",
    "    '''Get data from mimic Postgres database for transfused and non-transfused patients.\n",
    "    OUTPUT: pandas dataframe with rows=admission, cols=hadmid, source (transfused or control), text.'''\n",
    "    \n",
    "    # transfused\n",
    "    xf_adm = pd.read_sql(\"\"\"select * from mimiciii.transfused_notes_unique\n",
    "    \"\"\", engine)\n",
    "    \n",
    "    xf_adm['source'] = 'transfusion'\n",
    "    \n",
    "    # control\n",
    "    ctrl_adm = pd.read_sql(\"\"\"select * from mimiciii.ctrl_notes_unique\n",
    "\"\"\", engine)\n",
    "    \n",
    "    ctrl_adm['source'] = 'control'\n",
    "\n",
    "    \n",
    "    mimic_notes = pd.concat([xf_adm, ctrl_adm])\n",
    "    mimic_notes = mimic_notes[['hadm_id', 'source', 'text']]\n",
    "    \n",
    "    return mimic_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21443"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xf, ct =get_adm_notes()\n",
    "len(xf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will run everything. takes about 6:12 hours on the large AWS instance\n",
    "+ Time to extract features: 6:12\n",
    "+ pulls notes\n",
    "+ remove masked pii string\n",
    "+ tokenize\n",
    "+ collocations (detect multi-word tokens)\n",
    "+ add additional vocab terms\n",
    "+ count vectorize using vocab we created in the first part (so, setting the n-grams, and all those params won't matter b/c we pass our own vocabulary) \n",
    "+ save in sections as pickle files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-26 01:15:52.441000\n",
      "Getting notes\n",
      "Text Features\n",
      "Creating tokens\n",
      "Done with tokens\n",
      "2019-08-26 01:37:39.470000\n",
      "trigram done\n",
      "2019-08-26 02:16:29.620000\n",
      "tetragram done\n",
      "2019-08-26 03:05:22.402000\n",
      "pentagram done\n",
      "2019-08-26 04:15:04.895000\n",
      "bigrams done\n",
      "2019-08-26 04:34:29.751000\n",
      "trigrams done\n",
      "2019-08-26 05:09:05.855000\n",
      "tetragrams done\n",
      "2019-08-26 05:58:31.219000\n",
      "2019-08-26 07:01:59.621000\n",
      "2391685\n",
      "2019-08-26 07:02:51.596000\n",
      "7422044\n",
      "Done with collocation detection\n",
      "Starting feature extraction\n",
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None,\n",
      "        vocabulary=[u'can have regular diet,', u'5 cmh2o rsbi: 61', u'double vision. functional status:', u'be constructed', u'5 cmh2o rsbi: 68', u's/p y stent placement .', u'glass opacities,', u'moist, trachea', u'glass opacities.', u'vs: t 98.4 hr', u'spo2>94.', u'sodium 17. magnesium', u'spo2>94%', u'k-...n', u'lumenal narrowing.', u'schalatter,', u'lumenal narrowing)', u'the overall appearance has not'])\n",
      "...Done extracting text features...\n",
      "(49331, 7422044)\n",
      "Time to extract features: 6:10:52.417000\n",
      "(49331, 7422044)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print datetime.now()\n",
    "    print \"Getting notes\"\n",
    "       \n",
    "    mimic_notes=get_adm_notes()\n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    print \"Text Features\"\n",
    "    \n",
    "    text_features = TextFeatures(corpus=mimic_notes.text.values)  \n",
    "    text_features.remove_masked_pii_string()  \n",
    "    \n",
    "    vocab = text_features.tokenize_and_phrase()\n",
    "    \n",
    "    r = list(text_features.get_ngram_counts(token_pattern='(?u)\\\\b\\w\\w+\\\\b', vocabulary=vocab))\n",
    "    \n",
    "    r.append(mimic_notes.hadm_id.values)\n",
    "    r.append(mimic_notes.source.values)\n",
    "    \n",
    "    print(\"...Done extracting text features...\")\n",
    "    print r[0].shape\n",
    "    print (\"Time to extract features: %s\" % (str(datetime.now() - startTime)))\n",
    "    \n",
    "    feature_pickle(r, 'D:\\\\vectorization_results') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}