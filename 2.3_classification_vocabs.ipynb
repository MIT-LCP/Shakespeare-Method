{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Classification Vocabulary for Transfused\n",
    "- run classification using Logistic Regression (l1 penalty) and filter out low \n",
    "- load cleaned Naive Bayes vocab from 2.1.3\n",
    "- load cleaned Logistic Regression (l2 penalty) from 2.2.3\n",
    "- run chi2 on full vocab and filter for terms p=<.05\n",
    "- put together in one df and save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import scale, LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, make_scorer, accuracy_score\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "from importlib_metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries = ['pandas','numpy','scikit-learn', 'scipy','matplotlib']\n",
    "print('last ran: ',datetime.now() )\n",
    "print(\"Python Version:\", sys.version[0:7])\n",
    "print( \"operating system:\", sys.platform)\n",
    "\n",
    "for lib in libraries:\n",
    "    print(lib + ' version: ' + version(lib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_unpickle(path):        \n",
    "    mat = []\n",
    "    for i in range(0,10):\n",
    "        with open(path+'textfeatures_mat'+str(i+1)+'.pickle', 'rb') as f:\n",
    "            mat.append(pickle.load(f, encoding='latin1'))\n",
    "    mat=vstack(mat)\n",
    "\n",
    "    q=[mat]\n",
    "    with open(path+'textfeatures_vocab.pickle', 'rb') as f:\n",
    "        vocab=pickle.load(f, encoding='latin1')\n",
    "        q.append(vocab)\n",
    "    with open(path+'textfeatures_id.pickle', 'rb') as f:\n",
    "        ids=pickle.load(f, encoding='latin1')\n",
    "        q.append(ids)\n",
    "    with open(path+'textfeatures_source.pickle', 'rb') as f:\n",
    "        source=pickle.load(f, encoding='latin1')\n",
    "        q.append(source)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_split(X, y):\n",
    "    \n",
    "    rs = StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.25, train_size=None)\n",
    "    \n",
    "    for train_index, test_index in rs.split(X,y):\n",
    "        print('TRAIN:', train_index, \"TEST:\", test_index)\n",
    "        \n",
    "    X_train = X[train_index,:]\n",
    "    X_test = X[test_index,:]\n",
    "    \n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=feature_unpickle(path)\n",
    "XX = r[0] # sparse document-term. matrix\n",
    "y = r[3] # label (transfused/ non-transfused)\n",
    "\n",
    "vocab = r[1]\n",
    "hadmids = r[2] # hadm_ids for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make train test split\n",
    "X_train, X_test, y_train, y_test = shuffle_split(XX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.summer):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "source": [
    "+ l2 penalty was calculated already in step 2.0, only need to redo for l1\n",
    "\n",
    "+ decision_function will give us confidence intervals per sample in predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LR(X_train, X_test, y_train, y_test, penalty = ['l1', 'l2']):\n",
    "    \n",
    "    LRmodel = LogisticRegression(verbose=1, penalty = penalty)\n",
    "    LRmodel.fit(X_train, y_train)\n",
    "    \n",
    "    print( \"Logistic regression Scoring on test set\")\n",
    "    \n",
    "    LR_pred = LRmodel.predict(X_test)\n",
    "    cr = classification_report(y_test, LR_pred)\n",
    "    \n",
    "    print (cr)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, LR_pred)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plot = plot_confusion_matrix(cm, classes=['Non','transfused'], normalize=False)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return LRmodel, LR_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_LR(X, y, penalty = ['l1', 'l2']):\n",
    "    \n",
    "    LRmodel = LogisticRegression(verbose=1, penalty = penalty)\n",
    "    LRmodel.fit(X, y)\n",
    "    \n",
    "    print( \"Logistic regression Scoring on test set\")\n",
    "    \n",
    "    LR_pred = LRmodel.predict(X)\n",
    "    cr = classification_report(y, LR_pred)\n",
    "    \n",
    "    print (cr)\n",
    "    \n",
    "    cm = confusion_matrix(y, LR_pred)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plot = plot_confusion_matrix(cm, classes=['non','transfsused'], normalize=False)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return LRmodel, LR_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_l2_model, lr_12_pred = train_LR(X_train, X_test, y_train, y_test, 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to train logistic regression on train/test/split\n",
    "lr_l1_model, lr_1l_pred = train_LR(X_train, X_test, y_train, y_test, 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train logistic regression on entire dataset (no train/test split)\n",
    "lr_l1_full_model, lr_l1_full_pred = full_LR(XX, y, 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_l2_full_model, lr_l2_full_pred = full_LR(XX, y, 'l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression top features\n",
    "def get_vocab_LR(model, matrix_vocab, thresh):\n",
    "    features = zip(matrix_vocab, model.coef_[0]) # zip vocabulary and model betas\n",
    "    df = pd.DataFrame(features).sort_values(by=1, ascending=False) # tail\n",
    "    \n",
    "    top = df.head(thresh)\n",
    "    \n",
    "    return top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NB Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the NB top 5k terms that have been cleaned (transfused terms removed, phrases concatenated)\n",
    "NB_mat = pd.read_csv(path + 'NB_top_4879_terms_only_dist.csv')\n",
    "NB_mat.head()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LR l2 vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the NB top 5k terms that have been cleaned (transfused terms removed, phrases concatenated)\n",
    "lr_l2_mat = pd.read_csv(path + 'LR_top_5000_terms_only.csv')\n",
    "lr_l2_mat.head()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab from all three models\n",
    "\n",
    "NBvocab = NB_mat#.loc[:,'feature']\n",
    "LRvocab_l2 = lr_l2_mat#.loc[:,'feature']\n",
    "LRvocab_l1 = get_vocab_LR(lr_l1_full_model, vocab, 5000)\n",
    "LRvocab_l1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRvocab_l1[1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of lasso coefs less than 0\n",
    "# does not actually get rid of anything b/c coefs are high for this classification\n",
    "# increasing to .2 b/c it's in 70%ish IQR\n",
    "LRvocab_l1 = LRvocab_l1[LRvocab_l1[1]>0.2]\n",
    "LRvocab_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRvocab_l1.rename(columns = {0: 'vocab', 1: 'LR_l1_coef'},inplace = True)\n",
    "LRvocab_l2.rename(columns = {'feature': 'vocab', 'coef': 'LR_l2_coef','total_hadmids':'LR_l2_total_hadmids','total_count_freq':'LR_l2_total_count_freq'},inplace = True)\n",
    "NBvocab.rename(columns = {'feature': 'vocab','ratio': 'NB_ratio', 'total_hadmids':'NB_total_hadmids','total_count_freq':'NB_total_count_freq'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best\n",
    "chi squared test on full dataset (not 10k vocab)  \n",
    "gives pval for every single feature (b/c we run on full vocab)  \n",
    "most are nan  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2)\n",
    "selector.fit(XX, y) # fit to entire datase\n",
    "scores = selector.pvalues_ # get all the features p values\n",
    "df = pd.DataFrame(scores) # put scores into df\n",
    "df['vocab'] = vocab # match to vocab from matrix\n",
    "df.dropna(inplace = True) # drop nulls - most of the select k best = null\n",
    "df.rename(columns = {0: 'chi2_pval_p_05'},inplace = True) \n",
    "df = df[df['chi2_pval_p_05'] <= 0.05] # keeping only p values less than or equal to 0.05\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Vocab & Chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine vocabulary from the top coefficients from logistic regression (ridge and lasso), Naive Bayes, and select K Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del alldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf = NBvocab.merge(LRvocab_l2, on = 'vocab', how ='outer') # merge NB, ridge LR\n",
    "alldf = alldf.merge(LRvocab_l1, on = 'vocab', how = 'outer') # merge lasso LR\n",
    "alldf = alldf.merge(df, on ='vocab', how = 'outer') # merge chi2 significant\n",
    "#alldf.drop(columns = [0, 1], inplace = True)\n",
    "alldf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(alldf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf.to_csv(path + 'final_classification_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, \n",
    "               y_test, \n",
    "               Y_pred, \n",
    "               path,\n",
    "               modeltype = ['LR', 'NB', 'RF'], \n",
    "               save_model = False,\n",
    "               matrix_vocab = vocab,\n",
    "               thresh = 5000):\n",
    "    \n",
    "    \n",
    "    # creating base path for saving all pieces\n",
    "    basepath = path + modeltype + '/' + modeltype\n",
    "    \n",
    "    if save_model == True:\n",
    "        # saving model\n",
    "        with open(basepath + '.pickle', 'wb') as picklefile:  \n",
    "            pickle.dump(model, picklefile)\n",
    "    \n",
    "    # remaking and saving classification report as dataframe\n",
    "    cr = classification_report(y_test, Y_pred, output_dict=True)\n",
    "    cr = pd.DataFrame(cr).transpose()\n",
    "    cr.to_csv(basepath + '_classification_report.csv')\n",
    "    \n",
    "    # remake and save confusion matrix\n",
    "    cm = confusion_matrix(y_test, Y_pred)\n",
    "    #fig = plt.figure(figsize=(8, 8))\n",
    "    plot = plot_confusion_matrix(cm, classes=['pre','post'], normalize=False)\n",
    "    plt.savefig(basepath + '_confusion_matrix.svg')\n",
    "    \n",
    "    # save vocab\n",
    "    if modeltype == 'LR':\n",
    "        topwords = get_vocab(model, matrix_vocab, thresh)\n",
    "        topwords.to_csv(basepath + '_topwords.csv')\n",
    "    \n",
    "    #return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_model(LRmodel, y_test, LR_pred, \n",
    "                      path = path, \n",
    "                      modeltype = 'LR', save_model = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python376jvsc74a57bd0ae563281e74dc9efe7b324d32dcd01f8eedf5b1e4f387da8c920ce510da32e5f",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}